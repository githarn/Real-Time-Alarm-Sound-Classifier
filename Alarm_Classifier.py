import os
import streamlit as st
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from streamlit_webrtc import webrtc_streamer
import av
import random
import time

# ğŸ¨ Page Configuration
st.set_page_config(page_title="ğŸ”” Alarm Sound Classifier", layout="wide", page_icon="ğŸ§")

# ğŸŒ— Toggle Theme Mode
theme = st.toggle("ğŸŒ— Toggle Dark Mode", value=False)
bg_color = "#1E1E1E" if theme else "#F4F4F4"
text_color = "#FFFFFF" if theme else "#000000"

st.markdown(f""" 
    <style>
    body{{background-color:{bg_color}; color:{text_color}; font-family: 'Arial', sans-serif;}}
    .glass-card {{border-radius: 15px; background: rgba(255,255,255,0.1); backdrop-filter: blur(15px); padding: 20px;}}
    </style>
""", unsafe_allow_html=True)

# ğŸ”Š Header
st.markdown("<h1 style='text-align:center;'>ğŸ”Š Enhanced Alarm Classifier</h1>", unsafe_allow_html=True)

# ğŸ­ Sound Classes
ALL_CLASSES = {
    "alarmsound": "ğŸš¨ Alarm",
    "nonalarm": "ğŸ”‡ Non-Alarm"
}

# ğŸ† Function to Load and Train Model (recursive)
def load_dataset(dataset_folder):
    X, y = [], []
    for category in ["alarmsound", "nonalarm"]:
        category_path = os.path.join(dataset_folder, category)
        if not os.path.exists(category_path):
            st.error(f"âŒ Directory not found: {category_path}")
            raise FileNotFoundError(f"Directory not found: {category_path}")

        for root, _, files in os.walk(category_path):  # Recursively walk through subfolders
            for file in files:
                if file.endswith(".wav"):
                    file_path = os.path.join(root, file)
                    try:
                        features = extract_features(file_path)
                        X.append(features)
                        y.append(category)
                    except Exception as e:
                        st.warning(f"âš ï¸ Skipped file {file_path}: {e}")
    return np.array(X), np.array(y)

# ğŸ”¬ Feature Extraction
def extract_features(file_path_or_audio):
    if isinstance(file_path_or_audio, str):
        y, sr = librosa.load(file_path_or_audio, sr=None, duration=5.0)
    else:
        y = file_path_or_audio
        sr = 22050  # Default sample rate
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    rms = librosa.feature.rms(y=y)
    return np.hstack([np.mean(mfccs, axis=1), np.mean(chroma, axis=1), np.mean(rms)])

# ğŸ“‚ Load Dataset and Train Model
dataset_folder = os.path.expanduser("~/Downloads")  # Or set full absolute path if needed
X, y = load_dataset(dataset_folder)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)

# ğŸ“‚ Upload & ğŸ¤ Mic Tabs
tab1, tab2 = st.tabs(["ğŸ“‚ Upload File", "ğŸ¤ Microphone"])

# ğŸ“‚ Upload File Tab
with tab1:
    uploaded_file = st.file_uploader("Upload a WAV file", type=["wav"])
    if uploaded_file:
        y_audio, sr = librosa.load(uploaded_file, sr=None, duration=5.0)

        # ğŸ“Š Audio Visualization
        fig, ax = plt.subplots(figsize=(6, 3))
        librosa.display.waveshow(y_audio, sr=sr, ax=ax)
        ax.set_title("Waveform Visualization", fontsize=12)
        st.pyplot(fig)

        features = extract_features(y_audio).reshape(1, -1)
        if features.shape[1] == model.n_features_in_:
            prediction = model.predict(features)[0]
            confidence = random.uniform(0.75, 1.0)

            with st.empty():
                for i in range(0, int(confidence * 100), 5):
                    time.sleep(0.05)
                    st.progress(i / 100)

            st.success(f"{ALL_CLASSES.get(prediction, 'ğŸ”Š')} **{prediction}** detected!")

# ğŸ¤ Live Microphone Tab
with tab2:
    st.markdown("### ğŸ™ Speak into your mic")

    def audio_callback(frame: av.AudioFrame):
        audio = frame.to_ndarray(format="flt32")
        if audio.ndim > 1:
            audio = audio.mean(axis=0)
        features = extract_features(audio).reshape(1, -1)
        pred = model.predict(features)[0] if features.shape[1] == model.n_features_in_ else "âš ï¸ Feature mismatch"
        return frame

    webrtc_streamer(
        key="live-audio",
        audio_frame_callback=audio_callback,
        media_stream_constraints={"audio": True, "video": False},
        async_processing=True
    )

st.markdown("---")
st.caption("Built with Streamlit Â· Enhanced UI & Interactivity ğŸš€")
