import streamlit as st
import numpy as np
import librosa
from sklearn.neighbors import KNeighborsClassifier
from streamlit_webrtc import webrtc_streamer
import av

# Page configuration
st.set_page_config(page_title="ğŸ”” Real-Time Alarm Classifier", layout="centered", page_icon="ğŸ”Š")

# App title
st.markdown("<h1 style='text-align:center;'>ğŸ”Š Real-Time Alarm Sound Classifier</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align:center;'>Upload a WAV file or use your mic to classify alarms and noises in real-time.</p>", unsafe_allow_html=True)

# Sidebar info
with st.sidebar:
    st.header("â„¹ï¸ How to Use")
    st.markdown("""
    - Upload a `.wav` file under 5 seconds **OR** use your microphone.
    - The model classifies the sound into:
      - ğŸ”¥ Fire alarm
      - ğŸ›ï¸ Buzzer
      - ğŸš¨ Smoke detector
      - â° Timer alarm
      - ğŸšª Opening door
      - ğŸ¶ Barking
      - ğŸ’§ Water
      - ğŸšœ Lawn mower
    - Trained on demo data. Real-time classification may vary.
    """)

# Dummy model
CLASSES = ["Fire alarm", "Buzzer", "Smoke detector", "Timer alarm", "Opening door", "Barking", "Water", "Lawn mower"]

def dummy_train_model():
    X = np.random.rand(len(CLASSES)*20, 26)
    y = np.repeat(CLASSES, 20)
    model = KNeighborsClassifier(n_neighbors=3)
    model.fit(X, y)
    return model

model = dummy_train_model()

# Feature extraction
def extract_features(y, sr):
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    rms = librosa.feature.rms(y=y)
    return np.hstack([np.mean(mfccs, axis=1), np.mean(chroma, axis=1), np.mean(rms)])

# Tabs for Upload vs. Live
tab1, tab2 = st.tabs(["ğŸ“‚ Upload WAV File", "ğŸ¤ Live Microphone Input"])

# WAV Upload
with tab1:
    uploaded_file = st.file_uploader("Upload a WAV file (max ~5 sec)", type=["wav"])
    if uploaded_file:
        with st.spinner("ğŸ” Analyzing..."):
            y, sr = librosa.load(uploaded_file, sr=None, duration=5.0)
            features = extract_features(y, sr).reshape(1, -1)
            if features.shape[1] == model.n_features_in_:
                prediction = model.predict(features)[0]
                st.success(f"ğŸ¯ Prediction: **{prediction}**")
            else:
                st.error("âš ï¸ Could not extract correct feature size.")

# Live mic
with tab2:
    st.markdown("### ğŸ”Š Start speaking to classify")
    st.caption("Ensure microphone permission is granted in your browser.")
    if "live_prediction" not in st.session_state:
        st.session_state["live_prediction"] = "Listening..."

    def audio_callback(frame: av.AudioFrame):
        audio = frame.to_ndarray(format="flt32")
        if audio.ndim > 1:
            audio = audio.mean(axis=0)
        sr = frame.sample_rate
        features = extract_features(audio, sr).reshape(1, -1)
        if features.shape[1] == model.n_features_in_:
            st.session_state["live_prediction"] = model.predict(features)[0]
        else:
            st.session_state["live_prediction"] = "âš ï¸ Feature mismatch"
        return frame

    webrtc_ctx = webrtc_streamer(
        key="audio-stream",
        audio_frame_callback=audio_callback,
        media_stream_constraints={"audio": True, "video": False},
        async_processing=True,
    )

    st.info(f"ğŸ”” Detected: **{st.session_state['live_prediction']}**")

# Footer
st.markdown("---")
st.caption("ğŸš§ Demo version â€” fine-tuning and training on real-world data will improve accuracy.")
